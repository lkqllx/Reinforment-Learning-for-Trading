{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #Only use GPU 0\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congifures\n",
    "ticker = \"510050.SH\"\n",
    "pastDays = 400\n",
    "lookbackNumWindowsForSingals = 20\n",
    "\n",
    "output_dir = \"./\"\n",
    "featureDir = \"./Features/\"\n",
    "readInCols = [\"Date\", \"VWAP\", \"todayOpen\", \"zt\", \"zt0\", \"zt1\", \"zt2\", \"zt3\", \"zt4\"]\n",
    "cols = ['ticker', 'date', 'delta', 'nextCloseToOpen', 'nextDayOpen','TR', 'SR', \n",
    "        'nDCtoCPnL', 'nDCtoCCumuPnL', 'nDCtoCPnLMinusTC', 'nDCtoCCumuPnLMinusTC']\n",
    "\n",
    "startIdx = 4\n",
    "n_steps = 20\n",
    "n_inputs = 5\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 128\n",
    "n_hidden_3 = 128\n",
    "n_hidden_4 = 16\n",
    "n_neurons = 1\n",
    "n_outputs = 1\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 200\n",
    "\n",
    "c = 0.0002\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defind reset function for neural network\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load feature data\n",
    "features = {}\n",
    "features[ticker] = pd.read_csv(featureDir + \"features_\" + ticker + \".csv\", \n",
    "                               usecols=readInCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load date information\n",
    "featureData = features[ticker]\n",
    "dates = featureData.Date\n",
    "date_index = np.unique(dates)\n",
    "date_num = date_index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty output file dataframe\n",
    "outputFile = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"z:0\", shape=(?, 20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Initalize trading PnL\n",
    "cumuPnL = 0\n",
    "cumuPnLMinusTC = 0\n",
    "\n",
    "# Go through All the date\n",
    "for i in range(date_num - pastDays - 1):\n",
    "    if i == 1:\n",
    "        break\n",
    "    \n",
    "    # Get the train data and the next day data\n",
    "    dataPd = featureData[i: i + pastDays]\n",
    "    dataNextD = featureData[i + pastDays: i + pastDays + 1] \n",
    "    \n",
    "    # Get features, zt0, and todayOpen\n",
    "    F_train = dataPd.iloc[:int(len(dataPd)/n_steps) * n_steps, \n",
    "                          startIdx:(n_inputs + startIdx)].values\n",
    "    z_train = dataPd.zt.values[:int(len(dataPd)/n_steps) * n_steps]\n",
    "    p_train= dataPd.todayOpen.values[:int(len(dataPd)/n_steps) * n_steps]\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    # Build data structures for f, z and p\n",
    "    f = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"input\")\n",
    "    z = tf.placeholder(tf.float32, shape=(None, n_steps, n_outputs), name=\"z\")\n",
    "    p = tf.placeholder(tf.float32, shape=(None, n_steps, n_outputs), name=\"p\")\n",
    "    \n",
    "    # Build neural network layers\n",
    "    with tf.name_scope(\"DNN\"):\n",
    "        hidden_1 = tf.layers.dense(f, n_hidden_1, activation=tf.nn.selu, name=\"hidden1\")\n",
    "        hidden_2 = tf.layers.dense(hidden_1, n_hidden_2, activation=tf.nn.selu, name=\"hidden2\")\n",
    "        hidden_3 = tf.layers.dense(hidden_2, n_hidden_3, activation=tf.nn.selu, name=\"hidden3\")\n",
    "        hidden_4 = tf.layers.dense(hidden_3, n_hidden_4, activation=tf.nn.selu, name=\"hidden4\")\n",
    "\n",
    "    # Define the final delta (position)\n",
    "    F = tf.reshape(hidden_4, [-1, n_steps, n_hidden_4])\n",
    "    cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=None, name=\"rnn\"), \n",
    "            output_size=n_outputs)\n",
    "    \n",
    "    deltaTemp, states = tf.nn.dynamic_rnn(cell, F, dtype=tf.float32)\n",
    "    delta = tf.nn.relu(deltaTemp, name=\"deltCalc\")\n",
    "    \n",
    "    # Define the result of the return every day (R) and the total return (U)\n",
    "    print(z)\n",
    "    R = tf.pad(delta[:, :(n_steps-1), :] * z[:, 1:(n_steps), :] \n",
    "               - tf.abs(delta[:, 1:n_steps, :] - delta[:, :(n_steps-1), :]) \n",
    "               * p[:, 1:(n_steps)] * c, paddings=[[0, 0], [1, 0], [0, 0]])\n",
    "    U = tf.reduce_mean(tf.reduce_sum(R, axis=1))\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    comp_grad = optimizer.compute_gradients(-U)\n",
    "    \n",
    "    optimizer_gradient = optimizer.apply_gradients(comp_grad)\n",
    "    \n",
    "    # Define total return and sharpe ratio\n",
    "    TR = tf.reduce_sum(R)\n",
    "    SR = tf.reduce_mean(R) / (tf.sqrt(tf.nn.moments(tf.reshape(R, [-1]), \n",
    "                                                    axes=0)[1]) + 1e-10) * np.sqrt(252)\n",
    "    \n",
    "    # Initialize and define Saver\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Train and test the model\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(F_train.shape[0] // batch_size):\n",
    "                f_batch = F_train[iteration*batch_size: (iteration+1)*batch_size, :]\n",
    "                z_batch = z_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                p_batch = p_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "                z_batch = z_batch.reshape((-1, n_steps, n_outputs))\n",
    "                p_batch = p_batch.reshape((-1, n_steps, n_outputs))\n",
    "                sess.run(optimizer_gradient, feed_dict={f: f_batch, z: z_batch, p: p_batch})\n",
    "\n",
    "        z_input = z_train.reshape((-1, n_steps, n_outputs))\n",
    "        p_input = p_train.reshape((-1, n_steps, n_outputs))\n",
    "\n",
    "        delta_sanityCheck = delta.eval(feed_dict={f: F_train, z: z_input, p:p_input}).flatten()\n",
    "        U_sanityCheck = U.eval(feed_dict={f: F_train, z: z_input, p: p_input}).flatten()\n",
    "        TR_sanityCheck = SR.eval(feed_dict={f: F_train, z: z_input, p: p_input}).flatten()\n",
    "        SR_sanityCheck = SR.eval(feed_dict={f: F_train, z: z_input, p: p_input}).flatten()\n",
    "        nextDayPnL = delta_sanityCheck.flatten()[-1] * dataNextD[\"zt\"].values[-1]\n",
    "        \n",
    "        if i == 0:\n",
    "            nextDayPnLMinusTC = nextDayPnL - np.abs(delta_sanityCheck.flatten()[-1]) * c\n",
    "        else:\n",
    "            nextDayPnLMinusTC = nextDayPnL - np.abs(delta_sanityCheck.flatten()[-1] - delta_sanityCheck.flatten()[-2]) * c\n",
    "            \n",
    "        cumuPnLMinusTC += nextDayPnLMinusTC\n",
    "        resultForThisDay = [ticker, str(date_index[i+pastDays]), \n",
    "                            delta_sanityCheck.flatten()[-1], dataNextD[\"zt\"].values[-1], \n",
    "                            dataNextD[\"todayOpen\"].values[-1], TR_sanityCheck[-1], \n",
    "                            SR_sanityCheck[-1], nextDayPnL, cumuPnL, nextDayPnLMinusTC, \n",
    "                            cumuPnLMinusTC]\n",
    "        outputFile = outputFile.append(pd.DataFrame(data=[resultForThisDay], columns=cols))\n",
    "        # print(resultForThisDay)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
